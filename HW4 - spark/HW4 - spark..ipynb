{
  "metadata": {
    "name": "HW4 - spark",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql._\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions._\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val spark \u003d SparkSession.builder()\r\n    // адрес мастера\r\n    .master(\"local[*]\")\r\n    // имя приложения в интерфейсе спарка\r\n    .appName(\"made-demo\")\r\n//     .config(\"spark.executor.memory\",  \"2g\")\r\n//     .config(\"spark.executor.cores\", \"2\")\r\n//     .config(\"spark.driver.memory\", \"2g\")\r\n    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\r\n    .option(\"header\", \"true\")\r\n    .option(\"inferSchema\", \"true\")\r\n    .option(\"sep\", \",\")\r\n    .csv(\"/my_new_folder/tripadvisor_hotel_reviews.csv\")\r\n\r\ndf.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 1. Приводим строки к нижнему регистру и удаляем все символы кроме строчных латинскх букв\nval df2 \u003d df.select(regexp_replace(lower(col(\"Review\")), \"[^a-z ]\", \"\").as(\"Clean_string\"))\ndf2.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 2. Разбиваем строку по пробельным символам\nvar df3 \u003d df2.select(split(col(\"Clean_string\"),\" +\").as(\"Array\"))\ndf3.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 3. Удаляем некоторые самые популярные стоп-слова, в том числе пустые элементы\nval stopWords \u003d Array(\"\", \"did\", \"got\", \"lot\", \"no\", \"nt\", \"th\", \"not\")\ndf3 \u003d df3.withColumn(\"Array\", array_except(df3(\"Array\"), lit(stopWords)))\ndf3.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 4. Создаем новый столбец с длинной слов в предложении\nvar df4 \u003d df3.select(\n    col(\"Array\"),\n    size(col(\"Array\")).as(\"total_word_sentence\")\n).orderBy(\"total_word_sentence\")\ndf4.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 5. Пронумеруем все предложения\nval windowSpec \u003d Window.partitionBy().orderBy(\"Array\")\ndf4 \u003d df4.withColumn(\"row_number\",row_number.over(windowSpec))\ndf4.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 6. Превратим столбец с массивами слов в столбец с отдельными словами\nvar df5 \u003d df4.select(\n    col(\"row_number\"),\n    col(\"total_word_sentence\"), \n    explode(col(\"Array\"))\n    )\ndf5.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 7. Считаем сколько раз встретилось слово в одном предложении\nvar df6_word \u003d df5.groupBy(\"col\", \"row_number\", \"total_word_sentence\").agg(count(\"col\").as(\"word_sentence\"))\ndf6_word.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 8. Считаем в скольких предложениях (документах) встретилось слово\nvar df6_documents \u003d df5.dropDuplicates().groupBy(\"col\").agg(count(\"row_number\").as(\"documents\"))\ndf6_documents \u003d df6_documents.orderBy(col(\"documents\").desc).limit(100)\ndf6_documents.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 9. Соединяем две таблицы, полученные на предыдущих двух шагах\nvar df6 \u003d df6_documents.join(df6_word, \"col\")\ndf6.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 10. Считаем общее количество предложений (документов) и вычисляем TF-IDF\nval total_documents \u003d df.count()\ndf6 \u003d df6.withColumn(\"TF-IDF\", (col(\"word_sentence\") * total_documents) / (col(\"total_word_sentence\") * col(\"documents\")))\ndf6.orderBy(col(\"TF-IDF\").desc).show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 11. Разворачиваем таблицу в ширину: строки - документы, столбцы - слова из списка топ-100 самых используемых\n// Также заполняем пропуски нулями\nvar df_final \u003d df6.select(\"row_number\", \"col\", \"TF-IDF\").groupBy(\"row_number\").pivot(\"col\").sum(\"TF-IDF\")\ndf_final \u003d df_final.na.fill(value\u003d0).orderBy(\"row_number\")\nprintln(df_final.count())\nprintln(df_final.columns.size)\ndf_final.show(5)"
    }
  ]
}